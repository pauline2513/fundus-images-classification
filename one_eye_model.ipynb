{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run preprocess_functions.ipynb\n",
    "%run data_augmentation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve, auc, roc_auc_score, roc_curve, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_PATH = 'one_eye_images'\n",
    "AUGMENTED_DIRECTORY_PATH =\"original_and_augmented_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model for one eye input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_X, dataset_y, image_directory_path, transfrom=None, include_preprocess_function=False, mode='train'):\n",
    "        self.X = dataset_X\n",
    "        self.y = dataset_y\n",
    "        self.image_directory_path = image_directory_path\n",
    "        # self.grouped_by_id_data = self.dataset.groupby('patien_id') #groups in format of { patient_id: [indx1, indx2] }\n",
    "        if mode == 'train':\n",
    "            if include_preprocess_function:\n",
    "                self.transform = transfrom or transforms.Compose([\n",
    "                    # transforms.RandomHorizontalFlip(),\n",
    "                    # transforms.RandomRotation(degrees=90),\n",
    "                    transforms.Lambda(preprocess),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #values from image net, better for pretrained models, can be changed to dataset values\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transfrom or transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    # transforms.RandomHorizontalFlip(),\n",
    "                    # transforms.RandomRotation(degrees=90),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #values from image net, better for pretrained models, can be changed to dataset values\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = transfrom or transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #values from image net, better for pretrained models, can be changed to dataset values\n",
    "                ])\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # eye_image = cv2.imread(os.path.join(self.image_directory_path, self.X.iloc[index]['image_id']))\n",
    "        eye_image = Image.open(os.path.join(self.image_directory_path, self.X.iloc[index]['image_id']), 'r')\n",
    "        diagnosis = self.y.iloc[index][['diabetic_retinopathy', 'amd', 'hypertensive_retinopathy', \n",
    "                                                'normal_eye', 'glaucoma', 'cataract']].to_numpy(dtype=np.float32)\n",
    "        age = self.X.iloc[index]['patient_age']\n",
    "        sex = self.X.iloc[index]['patient_sex']\n",
    "        if self.transform:\n",
    "           eye_image = self.transform(eye_image)\n",
    "        \n",
    "        data = {\"eye_image\": eye_image,\n",
    "                \"diagnosis\": torch.tensor(diagnosis, dtype=torch.float32),\n",
    "                \"metadata\": torch.tensor(np.array([age, sex]), dtype=torch.float32)}\n",
    "        \n",
    "        return data['eye_image'], data['metadata'], data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VGG(nn.Module):\n",
    "#     def __init__(\n",
    "#         self, features: nn.Module, num_classes: int = 1000, init_weights: bool = True, dropout: float = 0.5\n",
    "#     ) -> None:\n",
    "#         super().__init__()\n",
    "#         self.features = features\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(512 * 7 * 7, 4096),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p=dropout),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p=dropout),\n",
    "#             nn.Linear(4096, num_classes),\n",
    "#         )\n",
    "#         if init_weights:\n",
    "#             for m in self.modules():\n",
    "#                 if isinstance(m, nn.Conv2d):\n",
    "#                     nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "#                     if m.bias is not None:\n",
    "#                         nn.init.constant_(m.bias, 0)\n",
    "#                 elif isinstance(m, nn.BatchNorm2d):\n",
    "#                     nn.init.constant_(m.weight, 1)\n",
    "#                     nn.init.constant_(m.bias, 0)\n",
    "#                 elif isinstance(m, nn.Linear):\n",
    "#                     nn.init.normal_(m.weight, 0, 0.01)\n",
    "#                     nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         x = self.features(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. vgg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkVGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkVGG, self).__init__()\n",
    "        \n",
    "        self.eye_input = models.vgg16(weights='VGG16_Weights.DEFAULT').requires_grad_(False)\n",
    "        self.eye_input = nn.Sequential(*list(self.eye_input.children())[:-1]) #removing the classifier layer \n",
    "        # for param in self.eye_input.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        # self.eye_input = nn.Sequential(\n",
    "            \n",
    "        # )\n",
    "            \n",
    "        self.metadata_input = nn.Sequential(\n",
    "            nn.Linear(2, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(25088 + 64, 4096), #4096\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 6),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, eye_image, metadata): \n",
    "        vgg = self.eye_input(eye_image)\n",
    "        vgg = torch.flatten(vgg, 1)\n",
    "        meta = self.metadata_input(metadata)\n",
    "\n",
    "        # display(vgg)\n",
    "        # display(meta)\n",
    "        concatenated = torch.cat((vgg, meta), dim=1)\n",
    "        result = self.classifier(concatenated)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_function, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    running_loss = 0.\n",
    "    model.train()\n",
    "    for batch, (image, metadata, diagnosis) in enumerate(dataloader): #(image, metadata, diagnosis)\n",
    "\n",
    "        prediction = model(image, metadata) #, metadata\n",
    "        # print(\"prediction\", prediction[0])\n",
    "        # print(\"diagnosis\", diagnosis[0])\n",
    "        loss = loss_function(prediction, diagnosis)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 2 == 0:\n",
    "            loss, current = loss.item(), batch * len(diagnosis)\n",
    "            print(f\"loss: {loss:>7f}, [{current:>5d}/{size:>5d}]\")\n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_validation(dataloader, model, loss_function):\n",
    "    val_loss = 0.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (image, metadata, diagnosis) in dataloader: #(image, metadata, diagnosis)\n",
    "            prediction = model(image, metadata) #, metadata\n",
    "            batch_val_loss = loss_function(prediction, diagnosis)\n",
    "            val_loss += batch_val_loss\n",
    "    return val_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_function):\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (image, metadata, diagnosis) in dataloader:\n",
    "            prediction = model(image, metadata)\n",
    "            batch_test_loss = loss_function(prediction, diagnosis)\n",
    "            test_loss += batch_test_loss.item()\n",
    "            print(test_loss)\n",
    "    return test_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"final_one_eye_dataset.csv\")\n",
    "classes = ['diabetic_retinopathy', 'amd', 'hypertensive_retinopathy', 'normal_eye', 'glaucoma', 'cataract']\n",
    "X = data[['image_id', 'patient_age', 'patient_sex']]\n",
    "y = data[classes]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_augmentation = X_train.join(y_train)\n",
    "# target_samples = y_train.value_counts().max()\n",
    "# augmented_df = augment_dataset(data_for_augmentation, target_samples, classes, DIRECTORY_PATH, AUGMENTED_DIRECTORY_PATH, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df = pd.read_csv(\"augmented_df.csv\")\n",
    "augmented_df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>patient_age</th>\n",
       "      <th>patient_sex</th>\n",
       "      <th>diabetic_retinopathy</th>\n",
       "      <th>amd</th>\n",
       "      <th>hypertensive_retinopathy</th>\n",
       "      <th>normal_eye</th>\n",
       "      <th>glaucoma</th>\n",
       "      <th>cataract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>4673_left_aug0.jpg</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6417</th>\n",
       "      <td>2137_left_aug6.jpg</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>1453_right_aug4.jpg</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>1421_left_aug1.jpg</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3508</th>\n",
       "      <td>img00742_aug8.jpg</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 image_id  patient_age  patient_sex  diabetic_retinopathy  \\\n",
       "367    4673_left_aug0.jpg         85.0            1                     1   \n",
       "6417   2137_left_aug6.jpg         44.0            2                     0   \n",
       "4289  1453_right_aug4.jpg         77.0            1                     0   \n",
       "4968   1421_left_aug1.jpg         60.0            1                     0   \n",
       "3508    img00742_aug8.jpg         79.0            1                     0   \n",
       "\n",
       "      amd  hypertensive_retinopathy  normal_eye  glaucoma  cataract  \n",
       "367     0                         0           0         0         0  \n",
       "6417    0                         0           0         0         1  \n",
       "4289    0                         0           0         1         0  \n",
       "4968    0                         0           0         1         0  \n",
       "3508    0                         1           0         0         0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_and_augmented_df = pd.concat([data_for_augmentation, augmented_df], ignore_index=True)\n",
    "X_train = original_and_augmented_df[['image_id', 'patient_age', 'patient_sex']]\n",
    "y_train = original_and_augmented_df[classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\polin\\AppData\\Local\\Temp\\ipykernel_14888\\1755045109.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['patient_age'] = age_scaler.fit_transform(X_train[['patient_age']])\n"
     ]
    }
   ],
   "source": [
    "age_scaler = StandardScaler()\n",
    "\n",
    "X_train['patient_age'] = age_scaler.fit_transform(X_train[['patient_age']])\n",
    "X_val['patient_age'] = age_scaler.transform(X_val[['patient_age']])\n",
    "X_test['patient_age'] = age_scaler.transform(X_val[['patient_age']])\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train, AUGMENTED_DIRECTORY_PATH)\n",
    "validation_dataset = CustomDataset(X_val, y_val, AUGMENTED_DIRECTORY_PATH, mode='test')\n",
    "test_dataset = CustomDataset(X_test, y_test, AUGMENTED_DIRECTORY_PATH, mode='test')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.796266, [    0/11334]\n",
      "loss: 5.413529, [  256/11334]\n",
      "loss: 5.883486, [  512/11334]\n",
      "loss: 1.818259, [  768/11334]\n",
      "loss: 1.505136, [ 1024/11334]\n",
      "loss: 1.398294, [ 1280/11334]\n",
      "loss: 1.444097, [ 1536/11334]\n",
      "loss: 1.471624, [ 1792/11334]\n",
      "loss: 1.278474, [ 2048/11334]\n",
      "loss: 1.296580, [ 2304/11334]\n",
      "loss: 1.213410, [ 2560/11334]\n",
      "loss: 1.295469, [ 2816/11334]\n",
      "loss: 1.182201, [ 3072/11334]\n",
      "loss: 1.217734, [ 3328/11334]\n",
      "loss: 1.261012, [ 3584/11334]\n",
      "loss: 1.222866, [ 3840/11334]\n",
      "loss: 1.156577, [ 4096/11334]\n",
      "loss: 1.241525, [ 4352/11334]\n",
      "loss: 1.078676, [ 4608/11334]\n",
      "loss: 1.294641, [ 4864/11334]\n",
      "loss: 1.161287, [ 5120/11334]\n",
      "loss: 1.211407, [ 5376/11334]\n",
      "loss: 1.121953, [ 5632/11334]\n",
      "loss: 1.079645, [ 5888/11334]\n",
      "loss: 1.185516, [ 6144/11334]\n",
      "loss: 1.012404, [ 6400/11334]\n",
      "loss: 1.054030, [ 6656/11334]\n",
      "loss: 1.086447, [ 6912/11334]\n",
      "loss: 1.035527, [ 7168/11334]\n",
      "loss: 1.157171, [ 7424/11334]\n",
      "loss: 1.012563, [ 7680/11334]\n",
      "loss: 0.952883, [ 7936/11334]\n",
      "loss: 1.159131, [ 8192/11334]\n",
      "loss: 1.027606, [ 8448/11334]\n",
      "loss: 0.922725, [ 8704/11334]\n",
      "loss: 0.931716, [ 8960/11334]\n",
      "loss: 0.965383, [ 9216/11334]\n",
      "loss: 0.955338, [ 9472/11334]\n",
      "loss: 0.943211, [ 9728/11334]\n",
      "loss: 1.072855, [ 9984/11334]\n",
      "loss: 0.939975, [10240/11334]\n",
      "loss: 0.907223, [10496/11334]\n",
      "loss: 0.810417, [10752/11334]\n",
      "loss: 0.921436, [11008/11334]\n",
      "loss: 0.931369, [ 6160/11334]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.007210, [    0/11334]\n",
      "loss: 0.955203, [  256/11334]\n",
      "loss: 0.835721, [  512/11334]\n",
      "loss: 0.822628, [  768/11334]\n",
      "loss: 0.985937, [ 1024/11334]\n",
      "loss: 0.949783, [ 1280/11334]\n",
      "loss: 0.809648, [ 1536/11334]\n",
      "loss: 0.879862, [ 1792/11334]\n",
      "loss: 0.687261, [ 2048/11334]\n",
      "loss: 0.675288, [ 2304/11334]\n",
      "loss: 0.906369, [ 2560/11334]\n",
      "loss: 0.838131, [ 2816/11334]\n",
      "loss: 0.756713, [ 3072/11334]\n",
      "loss: 0.806281, [ 3328/11334]\n",
      "loss: 0.717038, [ 3584/11334]\n",
      "loss: 0.822081, [ 3840/11334]\n",
      "loss: 0.640194, [ 4096/11334]\n",
      "loss: 0.844467, [ 4352/11334]\n",
      "loss: 0.821300, [ 4608/11334]\n",
      "loss: 0.910368, [ 4864/11334]\n",
      "loss: 0.610184, [ 5120/11334]\n",
      "loss: 0.902054, [ 5376/11334]\n",
      "loss: 0.746919, [ 5632/11334]\n",
      "loss: 0.783651, [ 5888/11334]\n",
      "loss: 0.884127, [ 6144/11334]\n",
      "loss: 0.667619, [ 6400/11334]\n",
      "loss: 0.831904, [ 6656/11334]\n",
      "loss: 0.669873, [ 6912/11334]\n",
      "loss: 0.904549, [ 7168/11334]\n",
      "loss: 0.758602, [ 7424/11334]\n",
      "loss: 0.664489, [ 7680/11334]\n",
      "loss: 0.654241, [ 7936/11334]\n",
      "loss: 0.814275, [ 8192/11334]\n",
      "loss: 0.817228, [ 8448/11334]\n",
      "loss: 0.712875, [ 8704/11334]\n",
      "loss: 0.744603, [ 8960/11334]\n",
      "loss: 0.842438, [ 9216/11334]\n",
      "loss: 0.571827, [ 9472/11334]\n",
      "loss: 0.760272, [ 9728/11334]\n",
      "loss: 0.561763, [ 9984/11334]\n",
      "loss: 0.563469, [10240/11334]\n",
      "loss: 0.579350, [10496/11334]\n",
      "loss: 0.735353, [10752/11334]\n",
      "loss: 0.712078, [11008/11334]\n",
      "loss: 0.596182, [ 6160/11334]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.632193, [    0/11334]\n",
      "loss: 0.905680, [  256/11334]\n",
      "loss: 0.496232, [  512/11334]\n",
      "loss: 0.704972, [  768/11334]\n",
      "loss: 0.786000, [ 1024/11334]\n",
      "loss: 0.605897, [ 1280/11334]\n",
      "loss: 0.764309, [ 1536/11334]\n",
      "loss: 0.564403, [ 1792/11334]\n",
      "loss: 0.663614, [ 2048/11334]\n",
      "loss: 0.412885, [ 2304/11334]\n",
      "loss: 0.525174, [ 2560/11334]\n",
      "loss: 0.547055, [ 2816/11334]\n",
      "loss: 0.538218, [ 3072/11334]\n",
      "loss: 0.563734, [ 3328/11334]\n",
      "loss: 0.754903, [ 3584/11334]\n",
      "loss: 0.527495, [ 3840/11334]\n",
      "loss: 0.461478, [ 4096/11334]\n",
      "loss: 0.639073, [ 4352/11334]\n",
      "loss: 0.539470, [ 4608/11334]\n",
      "loss: 0.685130, [ 4864/11334]\n",
      "loss: 0.647191, [ 5120/11334]\n",
      "loss: 0.531640, [ 5376/11334]\n",
      "loss: 0.506783, [ 5632/11334]\n",
      "loss: 0.608645, [ 5888/11334]\n",
      "loss: 0.840008, [ 6144/11334]\n",
      "loss: 0.546819, [ 6400/11334]\n",
      "loss: 0.438359, [ 6656/11334]\n",
      "loss: 0.506836, [ 6912/11334]\n",
      "loss: 0.559868, [ 7168/11334]\n",
      "loss: 0.589660, [ 7424/11334]\n",
      "loss: 0.568779, [ 7680/11334]\n",
      "loss: 0.656981, [ 7936/11334]\n",
      "loss: 0.614965, [ 8192/11334]\n",
      "loss: 0.728944, [ 8448/11334]\n",
      "loss: 0.545814, [ 8704/11334]\n",
      "loss: 0.575350, [ 8960/11334]\n",
      "loss: 0.554610, [ 9216/11334]\n",
      "loss: 0.623405, [ 9472/11334]\n",
      "loss: 0.548264, [ 9728/11334]\n",
      "loss: 0.591923, [ 9984/11334]\n",
      "loss: 0.569682, [10240/11334]\n",
      "loss: 0.496967, [10496/11334]\n",
      "loss: 0.477949, [10752/11334]\n",
      "loss: 0.623699, [11008/11334]\n",
      "loss: 0.594640, [ 6160/11334]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.525133, [    0/11334]\n",
      "loss: 0.413752, [  256/11334]\n",
      "loss: 0.461330, [  512/11334]\n",
      "loss: 0.423023, [  768/11334]\n",
      "loss: 0.514154, [ 1024/11334]\n",
      "loss: 0.442257, [ 1280/11334]\n",
      "loss: 0.444610, [ 1536/11334]\n",
      "loss: 0.511568, [ 1792/11334]\n",
      "loss: 0.359831, [ 2048/11334]\n",
      "loss: 0.427841, [ 2304/11334]\n",
      "loss: 0.470676, [ 2560/11334]\n",
      "loss: 0.568621, [ 2816/11334]\n",
      "loss: 0.517276, [ 3072/11334]\n",
      "loss: 0.321470, [ 3328/11334]\n",
      "loss: 0.408275, [ 3584/11334]\n",
      "loss: 0.477081, [ 3840/11334]\n",
      "loss: 0.589528, [ 4096/11334]\n",
      "loss: 0.341306, [ 4352/11334]\n",
      "loss: 0.491184, [ 4608/11334]\n",
      "loss: 0.432007, [ 4864/11334]\n",
      "loss: 0.440259, [ 5120/11334]\n",
      "loss: 0.515305, [ 5376/11334]\n",
      "loss: 0.462017, [ 5632/11334]\n",
      "loss: 0.420701, [ 5888/11334]\n",
      "loss: 0.379333, [ 6144/11334]\n",
      "loss: 0.596971, [ 6400/11334]\n",
      "loss: 0.507267, [ 6656/11334]\n",
      "loss: 0.410023, [ 6912/11334]\n",
      "loss: 0.568935, [ 7168/11334]\n",
      "loss: 0.442291, [ 7424/11334]\n",
      "loss: 0.691781, [ 7680/11334]\n",
      "loss: 0.481156, [ 7936/11334]\n",
      "loss: 0.447465, [ 8192/11334]\n",
      "loss: 0.370970, [ 8448/11334]\n",
      "loss: 0.647155, [ 8704/11334]\n",
      "loss: 0.447456, [ 8960/11334]\n",
      "loss: 0.665921, [ 9216/11334]\n",
      "loss: 0.632693, [ 9472/11334]\n",
      "loss: 0.465292, [ 9728/11334]\n",
      "loss: 0.512563, [ 9984/11334]\n",
      "loss: 0.484144, [10240/11334]\n",
      "loss: 0.602838, [10496/11334]\n",
      "loss: 0.443326, [10752/11334]\n",
      "loss: 0.387271, [11008/11334]\n",
      "loss: 0.411413, [ 6160/11334]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.460270, [    0/11334]\n",
      "loss: 0.484108, [  256/11334]\n",
      "loss: 0.456087, [  512/11334]\n",
      "loss: 0.328539, [  768/11334]\n",
      "loss: 0.460359, [ 1024/11334]\n",
      "loss: 0.546425, [ 1280/11334]\n",
      "loss: 0.435315, [ 1536/11334]\n",
      "loss: 0.428432, [ 1792/11334]\n",
      "loss: 0.372335, [ 2048/11334]\n",
      "loss: 0.433997, [ 2304/11334]\n",
      "loss: 0.448852, [ 2560/11334]\n",
      "loss: 0.365280, [ 2816/11334]\n",
      "loss: 0.434561, [ 3072/11334]\n",
      "loss: 0.398743, [ 3328/11334]\n",
      "loss: 0.414019, [ 3584/11334]\n",
      "loss: 0.494162, [ 3840/11334]\n",
      "loss: 0.550798, [ 4096/11334]\n",
      "loss: 0.497167, [ 4352/11334]\n",
      "loss: 0.495588, [ 4608/11334]\n",
      "loss: 0.415396, [ 4864/11334]\n",
      "loss: 0.353711, [ 5120/11334]\n",
      "loss: 0.352742, [ 5376/11334]\n",
      "loss: 0.406288, [ 5632/11334]\n",
      "loss: 0.427524, [ 5888/11334]\n",
      "loss: 0.509851, [ 6144/11334]\n",
      "loss: 0.435769, [ 6400/11334]\n",
      "loss: 0.365768, [ 6656/11334]\n",
      "loss: 0.583049, [ 6912/11334]\n",
      "loss: 0.423734, [ 7168/11334]\n",
      "loss: 0.331521, [ 7424/11334]\n",
      "loss: 0.501731, [ 7680/11334]\n",
      "loss: 0.500587, [ 7936/11334]\n",
      "loss: 0.294702, [ 8192/11334]\n",
      "loss: 0.554588, [ 8448/11334]\n",
      "loss: 0.512051, [ 8704/11334]\n",
      "loss: 0.608624, [ 8960/11334]\n",
      "loss: 0.362984, [ 9216/11334]\n",
      "loss: 0.294366, [ 9472/11334]\n",
      "loss: 0.401436, [ 9728/11334]\n",
      "loss: 0.340790, [ 9984/11334]\n",
      "loss: 0.317520, [10240/11334]\n",
      "loss: 0.432431, [10496/11334]\n",
      "loss: 0.484745, [10752/11334]\n",
      "loss: 0.344872, [11008/11334]\n",
      "loss: 0.400153, [ 6160/11334]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.319647, [    0/11334]\n",
      "loss: 0.294737, [  256/11334]\n",
      "loss: 0.326382, [  512/11334]\n",
      "loss: 0.400549, [  768/11334]\n",
      "loss: 0.457058, [ 1024/11334]\n",
      "loss: 0.364067, [ 1280/11334]\n",
      "loss: 0.401204, [ 1536/11334]\n",
      "loss: 0.274961, [ 1792/11334]\n",
      "loss: 0.359416, [ 2048/11334]\n",
      "loss: 0.228303, [ 2304/11334]\n",
      "loss: 0.481124, [ 2560/11334]\n",
      "loss: 0.301618, [ 2816/11334]\n",
      "loss: 0.391033, [ 3072/11334]\n",
      "loss: 0.494545, [ 3328/11334]\n",
      "loss: 0.339415, [ 3584/11334]\n",
      "loss: 0.380243, [ 3840/11334]\n",
      "loss: 0.380132, [ 4096/11334]\n",
      "loss: 0.353760, [ 4352/11334]\n",
      "loss: 0.309248, [ 4608/11334]\n",
      "loss: 0.451329, [ 4864/11334]\n",
      "loss: 0.341824, [ 5120/11334]\n",
      "loss: 0.297777, [ 5376/11334]\n",
      "loss: 0.307565, [ 5632/11334]\n",
      "loss: 0.471603, [ 5888/11334]\n",
      "loss: 0.340578, [ 6144/11334]\n",
      "loss: 0.414783, [ 6400/11334]\n",
      "loss: 0.315305, [ 6656/11334]\n",
      "loss: 0.285909, [ 6912/11334]\n",
      "loss: 0.352995, [ 7168/11334]\n",
      "loss: 0.272410, [ 7424/11334]\n",
      "loss: 0.286748, [ 7680/11334]\n",
      "loss: 0.326860, [ 7936/11334]\n",
      "loss: 0.364211, [ 8192/11334]\n",
      "loss: 0.356080, [ 8448/11334]\n",
      "loss: 0.319048, [ 8704/11334]\n",
      "loss: 0.348216, [ 8960/11334]\n",
      "loss: 0.536261, [ 9216/11334]\n",
      "loss: 0.395936, [ 9472/11334]\n",
      "loss: 0.379499, [ 9728/11334]\n",
      "loss: 0.353987, [ 9984/11334]\n",
      "loss: 0.389002, [10240/11334]\n",
      "loss: 0.290342, [10496/11334]\n",
      "loss: 0.321256, [10752/11334]\n",
      "loss: 0.355773, [11008/11334]\n",
      "loss: 0.468789, [ 6160/11334]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.326110, [    0/11334]\n",
      "loss: 0.286389, [  256/11334]\n",
      "loss: 0.353497, [  512/11334]\n",
      "loss: 0.221236, [  768/11334]\n",
      "loss: 0.290361, [ 1024/11334]\n",
      "loss: 0.369185, [ 1280/11334]\n",
      "loss: 0.270896, [ 1536/11334]\n",
      "loss: 0.296178, [ 1792/11334]\n",
      "loss: 0.243038, [ 2048/11334]\n",
      "loss: 0.222490, [ 2304/11334]\n",
      "loss: 0.252146, [ 2560/11334]\n",
      "loss: 0.253383, [ 2816/11334]\n",
      "loss: 0.306551, [ 3072/11334]\n",
      "loss: 0.341552, [ 3328/11334]\n",
      "loss: 0.442495, [ 3584/11334]\n",
      "loss: 0.283698, [ 3840/11334]\n",
      "loss: 0.332864, [ 4096/11334]\n",
      "loss: 0.407731, [ 4352/11334]\n",
      "loss: 0.436749, [ 4608/11334]\n",
      "loss: 0.419029, [ 4864/11334]\n",
      "loss: 0.344522, [ 5120/11334]\n",
      "loss: 0.350792, [ 5376/11334]\n",
      "loss: 0.326085, [ 5632/11334]\n",
      "loss: 0.422553, [ 5888/11334]\n",
      "loss: 0.323567, [ 6144/11334]\n",
      "loss: 0.316754, [ 6400/11334]\n",
      "loss: 0.385903, [ 6656/11334]\n",
      "loss: 0.291283, [ 6912/11334]\n",
      "loss: 0.376919, [ 7168/11334]\n",
      "loss: 0.444437, [ 7424/11334]\n",
      "loss: 0.336086, [ 7680/11334]\n",
      "loss: 0.342526, [ 7936/11334]\n",
      "loss: 0.279503, [ 8192/11334]\n",
      "loss: 0.324359, [ 8448/11334]\n",
      "loss: 0.212288, [ 8704/11334]\n",
      "loss: 0.318368, [ 8960/11334]\n",
      "loss: 0.160659, [ 9216/11334]\n",
      "loss: 0.314187, [ 9472/11334]\n",
      "loss: 0.402313, [ 9728/11334]\n",
      "loss: 0.410652, [ 9984/11334]\n",
      "loss: 0.270567, [10240/11334]\n",
      "loss: 0.295343, [10496/11334]\n",
      "loss: 0.403182, [10752/11334]\n",
      "loss: 0.448500, [11008/11334]\n",
      "loss: 0.359523, [ 6160/11334]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.292056, [    0/11334]\n",
      "loss: 0.266305, [  256/11334]\n",
      "loss: 0.281541, [  512/11334]\n",
      "loss: 0.354134, [  768/11334]\n",
      "loss: 0.317328, [ 1024/11334]\n",
      "loss: 0.229132, [ 1280/11334]\n",
      "loss: 0.225934, [ 1536/11334]\n",
      "loss: 0.396911, [ 1792/11334]\n",
      "loss: 0.295183, [ 2048/11334]\n",
      "loss: 0.200644, [ 2304/11334]\n",
      "loss: 0.280407, [ 2560/11334]\n",
      "loss: 0.223917, [ 2816/11334]\n",
      "loss: 0.346756, [ 3072/11334]\n",
      "loss: 0.275432, [ 3328/11334]\n",
      "loss: 0.308135, [ 3584/11334]\n",
      "loss: 0.381221, [ 3840/11334]\n",
      "loss: 0.322250, [ 4096/11334]\n",
      "loss: 0.395093, [ 4352/11334]\n",
      "loss: 0.345348, [ 4608/11334]\n",
      "loss: 0.333812, [ 4864/11334]\n",
      "loss: 0.326936, [ 5120/11334]\n",
      "loss: 0.365034, [ 5376/11334]\n",
      "loss: 0.332451, [ 5632/11334]\n",
      "loss: 0.312014, [ 5888/11334]\n",
      "loss: 0.448951, [ 6144/11334]\n",
      "loss: 0.412184, [ 6400/11334]\n",
      "loss: 0.299503, [ 6656/11334]\n",
      "loss: 0.382973, [ 6912/11334]\n",
      "loss: 0.321338, [ 7168/11334]\n",
      "loss: 0.228779, [ 7424/11334]\n",
      "loss: 0.246794, [ 7680/11334]\n",
      "loss: 0.253448, [ 7936/11334]\n",
      "loss: 0.278465, [ 8192/11334]\n",
      "loss: 0.409678, [ 8448/11334]\n",
      "loss: 0.309534, [ 8704/11334]\n",
      "loss: 0.210776, [ 8960/11334]\n",
      "loss: 0.284593, [ 9216/11334]\n",
      "loss: 0.293644, [ 9472/11334]\n",
      "loss: 0.383753, [ 9728/11334]\n",
      "loss: 0.311349, [ 9984/11334]\n",
      "loss: 0.370033, [10240/11334]\n",
      "loss: 0.295585, [10496/11334]\n",
      "loss: 0.463020, [10752/11334]\n",
      "loss: 0.365974, [11008/11334]\n",
      "loss: 0.338616, [ 6160/11334]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.312186, [    0/11334]\n",
      "loss: 0.379326, [  256/11334]\n",
      "loss: 0.284790, [  512/11334]\n",
      "loss: 0.296811, [  768/11334]\n",
      "loss: 0.274977, [ 1024/11334]\n",
      "loss: 0.343723, [ 1280/11334]\n",
      "loss: 0.249808, [ 1536/11334]\n",
      "loss: 0.258127, [ 1792/11334]\n",
      "loss: 0.351134, [ 2048/11334]\n",
      "loss: 0.316956, [ 2304/11334]\n",
      "loss: 0.299955, [ 2560/11334]\n",
      "loss: 0.275913, [ 2816/11334]\n",
      "loss: 0.372819, [ 3072/11334]\n",
      "loss: 0.366004, [ 3328/11334]\n",
      "loss: 0.225845, [ 3584/11334]\n",
      "loss: 0.257601, [ 3840/11334]\n",
      "loss: 0.228201, [ 4096/11334]\n",
      "loss: 0.247095, [ 4352/11334]\n",
      "loss: 0.385415, [ 4608/11334]\n",
      "loss: 0.342034, [ 4864/11334]\n",
      "loss: 0.301848, [ 5120/11334]\n",
      "loss: 0.279468, [ 5376/11334]\n",
      "loss: 0.270567, [ 5632/11334]\n",
      "loss: 0.229682, [ 5888/11334]\n",
      "loss: 0.365279, [ 6144/11334]\n",
      "loss: 0.314196, [ 6400/11334]\n",
      "loss: 0.277903, [ 6656/11334]\n",
      "loss: 0.351772, [ 6912/11334]\n",
      "loss: 0.203614, [ 7168/11334]\n",
      "loss: 0.337701, [ 7424/11334]\n",
      "loss: 0.212457, [ 7680/11334]\n",
      "loss: 0.288595, [ 7936/11334]\n",
      "loss: 0.355445, [ 8192/11334]\n",
      "loss: 0.345878, [ 8448/11334]\n",
      "loss: 0.454530, [ 8704/11334]\n",
      "loss: 0.337070, [ 8960/11334]\n",
      "loss: 0.225966, [ 9216/11334]\n",
      "loss: 0.384598, [ 9472/11334]\n",
      "loss: 0.249926, [ 9728/11334]\n",
      "loss: 0.273243, [ 9984/11334]\n",
      "loss: 0.218211, [10240/11334]\n",
      "loss: 0.237753, [10496/11334]\n",
      "loss: 0.364375, [10752/11334]\n",
      "loss: 0.256837, [11008/11334]\n",
      "loss: 0.291520, [ 6160/11334]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.279460, [    0/11334]\n",
      "loss: 0.421481, [  256/11334]\n",
      "loss: 0.259486, [  512/11334]\n",
      "loss: 0.187154, [  768/11334]\n",
      "loss: 0.260515, [ 1024/11334]\n",
      "loss: 0.230862, [ 1280/11334]\n",
      "loss: 0.365525, [ 1536/11334]\n",
      "loss: 0.246459, [ 1792/11334]\n",
      "loss: 0.303621, [ 2048/11334]\n",
      "loss: 0.258462, [ 2304/11334]\n",
      "loss: 0.240709, [ 2560/11334]\n",
      "loss: 0.256666, [ 2816/11334]\n",
      "loss: 0.184856, [ 3072/11334]\n",
      "loss: 0.350034, [ 3328/11334]\n",
      "loss: 0.307253, [ 3584/11334]\n",
      "loss: 0.204306, [ 3840/11334]\n",
      "loss: 0.321156, [ 4096/11334]\n",
      "loss: 0.238950, [ 4352/11334]\n",
      "loss: 0.261480, [ 4608/11334]\n",
      "loss: 0.232542, [ 4864/11334]\n",
      "loss: 0.240428, [ 5120/11334]\n",
      "loss: 0.288378, [ 5376/11334]\n",
      "loss: 0.213872, [ 5632/11334]\n",
      "loss: 0.337349, [ 5888/11334]\n",
      "loss: 0.253919, [ 6144/11334]\n",
      "loss: 0.300963, [ 6400/11334]\n",
      "loss: 0.142252, [ 6656/11334]\n",
      "loss: 0.284067, [ 6912/11334]\n",
      "loss: 0.251545, [ 7168/11334]\n",
      "loss: 0.161403, [ 7424/11334]\n",
      "loss: 0.307224, [ 7680/11334]\n",
      "loss: 0.195167, [ 7936/11334]\n",
      "loss: 0.286770, [ 8192/11334]\n",
      "loss: 0.212657, [ 8448/11334]\n",
      "loss: 0.259989, [ 8704/11334]\n",
      "loss: 0.250936, [ 8960/11334]\n",
      "loss: 0.305360, [ 9216/11334]\n",
      "loss: 0.362194, [ 9472/11334]\n",
      "loss: 0.398335, [ 9728/11334]\n",
      "loss: 0.247535, [ 9984/11334]\n",
      "loss: 0.255900, [10240/11334]\n",
      "loss: 0.329084, [10496/11334]\n",
      "loss: 0.316658, [10752/11334]\n",
      "loss: 0.203095, [11008/11334]\n",
      "loss: 0.259348, [ 6160/11334]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.190143, [    0/11334]\n",
      "loss: 0.174547, [  256/11334]\n",
      "loss: 0.165837, [  512/11334]\n",
      "loss: 0.203225, [  768/11334]\n",
      "loss: 0.325904, [ 1024/11334]\n",
      "loss: 0.229804, [ 1280/11334]\n",
      "loss: 0.257594, [ 1536/11334]\n",
      "loss: 0.191048, [ 1792/11334]\n",
      "loss: 0.201383, [ 2048/11334]\n",
      "loss: 0.268005, [ 2304/11334]\n",
      "loss: 0.216125, [ 2560/11334]\n",
      "loss: 0.228017, [ 2816/11334]\n",
      "loss: 0.238739, [ 3072/11334]\n",
      "loss: 0.197011, [ 3328/11334]\n",
      "loss: 0.249849, [ 3584/11334]\n",
      "loss: 0.225295, [ 3840/11334]\n",
      "loss: 0.123564, [ 4096/11334]\n",
      "loss: 0.188313, [ 4352/11334]\n",
      "loss: 0.243442, [ 4608/11334]\n",
      "loss: 0.257747, [ 4864/11334]\n",
      "loss: 0.198181, [ 5120/11334]\n",
      "loss: 0.243074, [ 5376/11334]\n",
      "loss: 0.212655, [ 5632/11334]\n",
      "loss: 0.470334, [ 5888/11334]\n",
      "loss: 0.244655, [ 6144/11334]\n",
      "loss: 0.295571, [ 6400/11334]\n",
      "loss: 0.176390, [ 6656/11334]\n",
      "loss: 0.211514, [ 6912/11334]\n",
      "loss: 0.266437, [ 7168/11334]\n",
      "loss: 0.246091, [ 7424/11334]\n",
      "loss: 0.234222, [ 7680/11334]\n",
      "loss: 0.241121, [ 7936/11334]\n",
      "loss: 0.247809, [ 8192/11334]\n",
      "loss: 0.235280, [ 8448/11334]\n",
      "loss: 0.273424, [ 8704/11334]\n",
      "loss: 0.262142, [ 8960/11334]\n",
      "loss: 0.364754, [ 9216/11334]\n",
      "loss: 0.166232, [ 9472/11334]\n",
      "loss: 0.267581, [ 9728/11334]\n",
      "loss: 0.314488, [ 9984/11334]\n",
      "loss: 0.254516, [10240/11334]\n",
      "loss: 0.265431, [10496/11334]\n",
      "loss: 0.161433, [10752/11334]\n",
      "loss: 0.185350, [11008/11334]\n",
      "loss: 0.234825, [ 6160/11334]\n",
      "Epoch 12\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs_vgg16):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     epoch_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_vgg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     epoch_val_loss \u001b[38;5;241m=\u001b[39mtest_and_validation(validation_dataloader, model_vgg, loss_function)\n\u001b[0;32m     13\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(epoch_train_loss)\n",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_function, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 5\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiagnosis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#(image, metadata, diagnosis)\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#, metadata\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(\"prediction\", prediction[0])\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(\"diagnosis\", diagnosis[0])\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[4], line 41\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     39\u001b[0m sex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39miloc[index][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatient_sex\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 41\u001b[0m    eye_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43meye_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meye_image\u001b[39m\u001b[38;5;124m\"\u001b[39m: eye_image,\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiagnosis\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(diagnosis, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([age, sex]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)}\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meye_image\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagnosis\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:468\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    466\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    467\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:2138\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   2136\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(size)\n\u001b[1;32m-> 2138\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2140\u001b[0m     box \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs_vgg16 = 15\n",
    "learning_rate_vgg = 0.001\n",
    "model_vgg = NetworkVGG()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_vgg.parameters(), lr=learning_rate_vgg)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for t in range(epochs_vgg16):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    epoch_train_loss = train(train_dataloader, model_vgg, loss_function, optimizer)\n",
    "    epoch_val_loss =test_and_validation(validation_dataloader, model_vgg, loss_function)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_vgg.state_dict(), 'vgg_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3695705653576369,\n",
       " 0.7781428319684575,\n",
       " 0.5923326380467147,\n",
       " 0.4782548225327824,\n",
       " 0.4305681502551175,\n",
       " 0.3712513761573963,\n",
       " 0.3401051274176394,\n",
       " 0.3178074627779843,\n",
       " 0.29567769186550313,\n",
       " 0.27077615076906225,\n",
       " 0.2396045196592138]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.1102),\n",
       " tensor(1.0592),\n",
       " tensor(1.0453),\n",
       " tensor(1.0799),\n",
       " tensor(1.0433),\n",
       " tensor(1.2642),\n",
       " tensor(1.0555),\n",
       " tensor(1.1894),\n",
       " tensor(1.3154),\n",
       " tensor(1.2449),\n",
       " tensor(1.2217)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train_losses.pkl', 'wb') as file:\n",
    "    pickle.dump(train_losses, file)\n",
    "with open('val_losses.pkl', 'wb') as file:\n",
    "    pickle.dump(val_losses, file)\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(age_scaler, file)\n",
    "    \n",
    "torch.save(train_dataloader, 'train_dataloader.pth')\n",
    "torch.save(validation_dataloader, 'validation_dataloader.pth')\n",
    "torch.save(test_dataloader, 'test_dataloader.pth')\n",
    "torch.save(optimizer.state_dict(), 'optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
