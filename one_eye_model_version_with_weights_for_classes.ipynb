{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run preprocess_functions.ipynb\n",
    "%run data_augmentation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, precision_recall_curve, auc, roc_auc_score, roc_curve, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_PATH = 'one_eye_images'\n",
    "AUGMENTED_DIRECTORY_PATH =\"original_and_augmented_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model for one eye input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_X, dataset_y, image_directory_path, transfrom=None, include_preprocess_function=False, mode='train'):\n",
    "        self.X = dataset_X\n",
    "        self.y = dataset_y\n",
    "        self.image_directory_path = image_directory_path\n",
    "        # self.grouped_by_id_data = self.dataset.groupby('patien_id') #groups in format of { patient_id: [indx1, indx2] }\n",
    "        if mode == 'train':\n",
    "            if include_preprocess_function:\n",
    "                self.transform = transfrom or transforms.Compose([\n",
    "                    # transforms.RandomHorizontalFlip(),\n",
    "                    # transforms.RandomRotation(degrees=90),\n",
    "                    transforms.Lambda(lambda img: Image.fromarray(preprocess(np.array(img)))),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomRotation(degrees=50),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #values from image net, better for pretrained models, can be changed to dataset values\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transfrom or transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    # transforms.RandomHorizontalFlip(),\n",
    "                    # transforms.RandomRotation(degrees=90),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #values from image net, better for pretrained models, can be changed to dataset values\n",
    "                ])\n",
    "        else:\n",
    "            if include_preprocess_function:\n",
    "                self.transform = transfrom or transforms.Compose([\n",
    "                    transforms.Lambda(lambda img: Image.fromarray(preprocess(np.array(img)))),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transfrom or transforms.Compose([\n",
    "                        transforms.Resize((224, 224)),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #values from image net, better for pretrained models, can be changed to dataset values\n",
    "                    ])\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # eye_image = cv2.imread(os.path.join(self.image_directory_path, self.X.iloc[index]['image_id']))\n",
    "        eye_image = Image.open(os.path.join(self.image_directory_path, self.X.iloc[index]['image_id']), 'r')\n",
    "        diagnosis = self.y.iloc[index][['diabetic_retinopathy', 'amd', 'hypertensive_retinopathy', \n",
    "                                                'normal_eye', 'glaucoma', 'cataract']].to_numpy(dtype=np.float32)\n",
    "        age = self.X.iloc[index]['patient_age']\n",
    "        sex = self.X.iloc[index]['patient_sex']\n",
    "        if self.transform:\n",
    "           eye_image = self.transform(eye_image)\n",
    "        \n",
    "        data = {\"eye_image\": eye_image,\n",
    "                \"diagnosis\": torch.tensor(diagnosis, dtype=torch.float32),\n",
    "                \"metadata\": torch.tensor(np.array([age, sex]), dtype=torch.float32)}\n",
    "        \n",
    "        return data['eye_image'], data['metadata'], data['diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. vgg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkVGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkVGG, self).__init__()\n",
    "        \n",
    "        self.eye_input = models.vgg16(weights='VGG16_Weights.DEFAULT').requires_grad_(False)\n",
    "        self.eye_input = nn.Sequential(*list(self.eye_input.children())[:-1]) #removing the classifier layer \n",
    "        # for param in self.eye_input.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        # self.eye_input = nn.Sequential(\n",
    "            \n",
    "        # )\n",
    "            \n",
    "        self.metadata_input = nn.Sequential(\n",
    "            nn.Linear(2, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(25088 + 64, 4096), #4096\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 6),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, eye_image, metadata): \n",
    "        vgg = self.eye_input(eye_image)\n",
    "        vgg = torch.flatten(vgg, 1)\n",
    "        meta = self.metadata_input(metadata)\n",
    "\n",
    "        # display(vgg)\n",
    "        # display(meta)\n",
    "        concatenated = torch.cat((vgg, meta), dim=1)\n",
    "        result = self.classifier(concatenated)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_function, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    running_loss = 0.\n",
    "    model.train()\n",
    "    for batch, (image, metadata, diagnosis) in enumerate(dataloader): #(image, metadata, diagnosis)\n",
    "\n",
    "        prediction = model(image, metadata) #, metadata\n",
    "        # print(\"prediction\", prediction[0])\n",
    "        # print(\"diagnosis\", diagnosis[0])\n",
    "        loss = loss_function(prediction, diagnosis)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 2 == 0:\n",
    "            loss, current = loss.item(), batch * len(diagnosis)\n",
    "            print(f\"loss: {loss:>7f}, [{current:>5d}/{size:>5d}]\")\n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_validation(dataloader, model, loss_function):\n",
    "    val_loss = 0.\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (image, metadata, diagnosis) in dataloader: #(image, metadata, diagnosis)\n",
    "            prediction = model(image, metadata) #, metadata\n",
    "            batch_val_loss = loss_function(prediction, diagnosis)\n",
    "            val_loss += batch_val_loss\n",
    "            preds = torch.argmax(prediction, dim=1)\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(torch.argmax(diagnosis, dim=1).cpu().numpy())\n",
    "    avg_loss = val_loss / len(dataloader)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    print(f\"validation_loss: {avg_loss:>7f}, f1_score: {f1:>7f}\")\n",
    "    return avg_loss, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(dataloader, model, loss_function):\n",
    "#     num_batches = len(dataloader)\n",
    "#     test_loss = 0.\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for (image, metadata, diagnosis) in dataloader:\n",
    "#             prediction = model(image, metadata)\n",
    "#             batch_test_loss = loss_function(prediction, diagnosis)\n",
    "#             test_loss += batch_test_loss.item()\n",
    "#             print(test_loss)\n",
    "#     return test_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"final_one_eye_dataset.csv\")\n",
    "classes = ['diabetic_retinopathy', 'amd', 'hypertensive_retinopathy', 'normal_eye', 'glaucoma', 'cataract']\n",
    "X = data[['image_id', 'patient_age', 'patient_sex']]\n",
    "y = data[classes]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diabetic_retinopathy  amd  hypertensive_retinopathy  normal_eye  glaucoma  cataract\n",
       "0                     0    0                         1           0         0           3094\n",
       "1                     0    0                         0           0         0           2376\n",
       "0                     1    0                         0           0         0            468\n",
       "                      0    0                         0           0         1            292\n",
       "                           1                         0           0         0            277\n",
       "                           0                         0           1         0            268\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[classes].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_scaler = StandardScaler()\n",
    "\n",
    "X_train['patient_age'] = age_scaler.fit_transform(X_train[['patient_age']])\n",
    "X_val['patient_age'] = age_scaler.transform(X_val[['patient_age']])\n",
    "X_test['patient_age'] = age_scaler.transform(X_val[['patient_age']])\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train, DIRECTORY_PATH, include_preprocess_function=True)\n",
    "validation_dataset = CustomDataset(X_val, y_val, DIRECTORY_PATH, mode='test', include_preprocess_function=True)\n",
    "test_dataset = CustomDataset(X_test, y_test, DIRECTORY_PATH, mode='test', include_preprocess_function=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "    # Compute class weights using class indices\n",
    "y_train_indices = np.argmax(y_train, axis=1)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_indices), y=y_train_indices)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "    #criterion = nn.BCEWithLogitsLoss() # Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.845412, [    0/ 4065]\n",
      "loss: 6.779254, [  256/ 4065]\n",
      "loss: 5.933875, [  512/ 4065]\n",
      "loss: 3.051322, [  768/ 4065]\n",
      "loss: 2.319752, [ 1024/ 4065]\n",
      "loss: 1.700814, [ 1280/ 4065]\n",
      "loss: 1.901054, [ 1536/ 4065]\n",
      "loss: 1.310896, [ 1792/ 4065]\n",
      "loss: 1.481751, [ 2048/ 4065]\n",
      "loss: 1.581696, [ 2304/ 4065]\n",
      "loss: 1.563545, [ 2560/ 4065]\n",
      "loss: 1.536388, [ 2816/ 4065]\n",
      "loss: 1.741104, [ 3072/ 4065]\n",
      "loss: 1.448448, [ 3328/ 4065]\n",
      "loss: 1.248655, [ 3584/ 4065]\n",
      "loss: 1.562592, [ 3840/ 4065]\n",
      "validation_loss: 1.299685, f1_score: 0.304625\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.394958, [    0/ 4065]\n",
      "loss: 1.868749, [  256/ 4065]\n",
      "loss: 1.238630, [  512/ 4065]\n",
      "loss: 1.134365, [  768/ 4065]\n",
      "loss: 1.238057, [ 1024/ 4065]\n",
      "loss: 1.540298, [ 1280/ 4065]\n",
      "loss: 1.211996, [ 1536/ 4065]\n",
      "loss: 1.449643, [ 1792/ 4065]\n",
      "loss: 1.260749, [ 2048/ 4065]\n",
      "loss: 1.168565, [ 2304/ 4065]\n",
      "loss: 1.105432, [ 2560/ 4065]\n",
      "loss: 1.121161, [ 2816/ 4065]\n",
      "loss: 1.073504, [ 3072/ 4065]\n",
      "loss: 1.291015, [ 3328/ 4065]\n",
      "loss: 1.784320, [ 3584/ 4065]\n",
      "loss: 0.981322, [ 3840/ 4065]\n",
      "validation_loss: 1.260546, f1_score: 0.333181\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.154120, [    0/ 4065]\n",
      "loss: 1.029504, [  256/ 4065]\n",
      "loss: 1.245844, [  512/ 4065]\n",
      "loss: 1.477979, [  768/ 4065]\n",
      "loss: 1.223035, [ 1024/ 4065]\n",
      "loss: 1.186985, [ 1280/ 4065]\n",
      "loss: 1.140088, [ 1536/ 4065]\n",
      "loss: 0.975127, [ 1792/ 4065]\n",
      "loss: 1.425580, [ 2048/ 4065]\n",
      "loss: 1.351495, [ 2304/ 4065]\n",
      "loss: 1.092783, [ 2560/ 4065]\n",
      "loss: 1.144348, [ 2816/ 4065]\n",
      "loss: 1.047001, [ 3072/ 4065]\n",
      "loss: 1.196126, [ 3328/ 4065]\n",
      "loss: 1.404960, [ 3584/ 4065]\n",
      "loss: 0.825968, [ 3840/ 4065]\n",
      "validation_loss: 1.192975, f1_score: 0.457563\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.103110, [    0/ 4065]\n",
      "loss: 1.414821, [  256/ 4065]\n",
      "loss: 1.136140, [  512/ 4065]\n",
      "loss: 1.461610, [  768/ 4065]\n",
      "loss: 1.202702, [ 1024/ 4065]\n",
      "loss: 1.126879, [ 1280/ 4065]\n",
      "loss: 1.131155, [ 1536/ 4065]\n",
      "loss: 1.122761, [ 1792/ 4065]\n",
      "loss: 1.079253, [ 2048/ 4065]\n",
      "loss: 1.047320, [ 2304/ 4065]\n",
      "loss: 1.019331, [ 2560/ 4065]\n",
      "loss: 1.096507, [ 2816/ 4065]\n",
      "loss: 1.448861, [ 3072/ 4065]\n",
      "loss: 0.999280, [ 3328/ 4065]\n",
      "loss: 1.549474, [ 3584/ 4065]\n",
      "loss: 1.253384, [ 3840/ 4065]\n",
      "validation_loss: 1.173906, f1_score: 0.426094\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.236179, [    0/ 4065]\n",
      "loss: 1.208441, [  256/ 4065]\n",
      "loss: 1.029659, [  512/ 4065]\n",
      "loss: 1.184660, [  768/ 4065]\n",
      "loss: 1.370545, [ 1024/ 4065]\n",
      "loss: 1.020989, [ 1280/ 4065]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs_vgg16):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m     epoch_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_vgg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     epoch_val_loss, f1_score_val \u001b[38;5;241m=\u001b[39mtest_and_validation(validation_dataloader, model_vgg, loss_function)\n\u001b[0;32m     14\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(epoch_train_loss)\n",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_function, optimizer)\u001b[0m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (image, metadata, diagnosis) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader): \u001b[38;5;66;03m#(image, metadata, diagnosis)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#, metadata\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# print(\"prediction\", prediction[0])\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# print(\"diagnosis\", diagnosis[0])\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(prediction, diagnosis)\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 40\u001b[0m, in \u001b[0;36mNetworkVGG.forward\u001b[1;34m(self, eye_image, metadata)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# display(vgg)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# display(meta)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m concatenated \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((vgg, meta), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcatenated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\polin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs_vgg16 = 5\n",
    "learning_rate_vgg = 0.001\n",
    "model_vgg = NetworkVGG()\n",
    "loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model_vgg.parameters(), lr=learning_rate_vgg)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "f1_scores = []\n",
    "\n",
    "for t in range(epochs_vgg16):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    epoch_train_loss = train(train_dataloader, model_vgg, loss_function, optimizer)\n",
    "    epoch_val_loss, f1_score_val =test_and_validation(validation_dataloader, model_vgg, loss_function)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    f1_scores.append(f1_score_val)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sas(a, b, c):\n",
    "    c.append(1)\n",
    "    return \"wer\"\n",
    "\n",
    "a = 1\n",
    "b = 2\n",
    "c = []\n",
    "sas(a, b, c)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_vgg.state_dict(), 'vgg_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3695705653576369,\n",
       " 0.7781428319684575,\n",
       " 0.5923326380467147,\n",
       " 0.4782548225327824,\n",
       " 0.4305681502551175,\n",
       " 0.3712513761573963,\n",
       " 0.3401051274176394,\n",
       " 0.3178074627779843,\n",
       " 0.29567769186550313,\n",
       " 0.27077615076906225,\n",
       " 0.2396045196592138]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.1102),\n",
       " tensor(1.0592),\n",
       " tensor(1.0453),\n",
       " tensor(1.0799),\n",
       " tensor(1.0433),\n",
       " tensor(1.2642),\n",
       " tensor(1.0555),\n",
       " tensor(1.1894),\n",
       " tensor(1.3154),\n",
       " tensor(1.2449),\n",
       " tensor(1.2217)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train_losses.pkl', 'wb') as file:\n",
    "    pickle.dump(train_losses, file)\n",
    "with open('val_losses.pkl', 'wb') as file:\n",
    "    pickle.dump(val_losses, file)\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(age_scaler, file)\n",
    "    \n",
    "torch.save(train_dataloader, 'train_dataloader.pth')\n",
    "torch.save(validation_dataloader, 'validation_dataloader.pth')\n",
    "torch.save(test_dataloader, 'test_dataloader.pth')\n",
    "torch.save(optimizer.state_dict(), 'optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
